{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sazimov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 51566 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention', 'a', 'kingdom', 'for', 'a', 'stage', 'princes', 'to', 'act', 'and', 'monarchs', 'to', 'behold', 'the', 'swelling', 'scene', 'then', 'should', 'the', 'warlike', 'harry', 'like', 'himself', 'assume', 'the', 'port', 'of', 'mars', 'and', 'at', 'his', 'heels', 'leash', 'in', 'like', 'hounds', 'should', 'famine', 'sword', 'and', 'fire', 'crouch', 'for', 'employment', 'but', 'pardon', 'and', 'gentles', 'all', 'the', 'flat', 'unraised', 'spirits', 'that', 'have', 'dared', 'on', 'this', 'unworthy', 'scaffold', 'to', 'bring', 'forth', 'so', 'great', 'an', 'object', 'can', 'this', 'cockpit', 'hold', 'the', 'vasty', 'fields', 'of', 'france', 'or', 'may', 'we', 'cram', 'within', 'this', 'wooden', 'o', 'the', 'very', 'casques']\n"
     ]
    }
   ],
   "source": [
    "# Load, tokenize and process the data\n",
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')\n",
    "with open('./data/shakespeare.txt') as f:\n",
    "    data = f.read()\n",
    "data = re.sub(r'[,.!?;-]', '',data)\n",
    "data = nltk.word_tokenize(data)\n",
    "data = [ ch.lower() for ch in data if ch.isalpha()]\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5940\n",
      "Most frequent tokens:  [('the', 1521), ('and', 1394), ('i', 1271), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 753), ('you', 747), ('is', 629), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397), ('be', 395)]\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5940\n"
     ]
    }
   ],
   "source": [
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N, V)\n",
    "    \n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V, N)\n",
    "    \n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N, 1)\n",
    "    \n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V, 1)\n",
    "    \n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def softmax(z):\n",
    "    '''\n",
    "    Inputs: \n",
    "        z: output scores from the hidden layer\n",
    "    Outputs: \n",
    "        yhat: prediction (estimate of y)\n",
    "    '''\n",
    "    # Calculate yhat (softmax)\n",
    "    yhat = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    return yhat\n",
    "\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result\n",
    "\n",
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "\n",
    "    # Calculate h\n",
    "    h = np.dot(W1, x) + b1\n",
    "  \n",
    "    # Apply the relu on h, \n",
    "    # store the relu in h\n",
    "    h = relu(h)\n",
    "    \n",
    "    # Calculate z\n",
    "    z = np.dot(W2, h) + b2\n",
    "\n",
    "    return z, h\n",
    "\n",
    "# compute_cost: cross-entropy cost function\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "    \n",
    "    # cost function\n",
    "    epsilon = 0.000001\n",
    "    logprobs = np.multiply(np.log(yhat + epsilon), y)\n",
    "    cost = -1 / batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    # Compute l1 as W2^T (Yhat - Y)\n",
    "    # and re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient\n",
    "    l1 = np.dot(W2.T, yhat - y)\n",
    "\n",
    "    # Apply relu to l1\n",
    "    Sl1 = relu(l1)\n",
    "\n",
    "    # compute the gradient for W1\n",
    "    grad_W1 = np.dot(l1, x.T) / batch_size\n",
    "\n",
    "    # Compute gradient of W2\n",
    "    grad_W2 = np.dot(yhat - y, h.T) / batch_size\n",
    "    \n",
    "    # compute gradient for b1\n",
    "    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size\n",
    "\n",
    "    # compute gradient for b2\n",
    "    grad_b2 = np.sum(yhat - y, axis=1, keepdims=True) / batch_size\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    return np.sum(y==y_hat)/len(y)\n",
    "\n",
    "def print_acc(W1, W2, b1, b2, data_size):\n",
    "    print()\n",
    "    x, y = next(get_batches(data[:data_size+1], word2Ind, V, C, data_size)) #len(data)-4\n",
    "    z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "    yhat = softmax(z)\n",
    "    s = np.sum(yhat[:,0] == yhat[:,1])\n",
    "    if s != 0:\n",
    "        print(\"Error s != 0\")\n",
    "\n",
    "    s = y * yhat\n",
    "    print(\"y*yhat: \" + str(np.sum(s)))\n",
    "    \n",
    "    yhat_ind = np.argmax(yhat, axis=0)\n",
    "    y_ind = np.argmax(y, axis=0)\n",
    "    \n",
    "    acc = accuracy(y_ind, yhat_ind)\n",
    "    \n",
    "    print(\"Accuracy: \" + str(acc))\n",
    "    print()\n",
    "    \n",
    "def save_weights_to_disk(W1, W2, b1, b2, alpha='', batch_size=''):\n",
    "    print(\"Saving weights to disk\")\n",
    "    np.savetxt(f\"W1_{alpha}_{batch_size}.csv\", W1, delimiter=',')\n",
    "    np.savetxt(f\"W2_{alpha}_{batch_size}.csv\", W2, delimiter=',')\n",
    "    np.savetxt(f\"b1_{alpha}_{batch_size}.csv\", b1, delimiter=',')\n",
    "    np.savetxt(f\"b2_{alpha}_{batch_size}.csv\", b2, delimiter=',')\n",
    "\n",
    "def load_weights_from_disk(alpha='', batch_size=''):\n",
    "    print(\"Loading weights from disk\")\n",
    "    W1 = np.loadtxt(f\"W1_{alpha}_{batch_size}.csv\", delimiter=',')\n",
    "    W2 = np.loadtxt(f\"W2_{alpha}_{batch_size}.csv\", delimiter=',')\n",
    "    b1 = np.expand_dims(np.loadtxt(f\"b1_{alpha}_{batch_size}.csv\", delimiter=','), axis=1)\n",
    "    b2 = np.expand_dims(np.loadtxt(f\"b2_{alpha}_{batch_size}.csv\", delimiter=','), axis=1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(W1, W2, b1, b2, data, word2Ind, N, V, num_iters, batch_size, train_data_size,\n",
    "                     alpha, alpha_decay=False, random_seed=282, initialize_model=initialize_model, \n",
    "                     get_batches=get_batches, forward_prop=forward_prop, \n",
    "                     softmax=softmax, compute_cost=compute_cost, \n",
    "                     back_prop=back_prop):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: your implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_prop: your implementation of the function to perform forward propagation\n",
    "        softmax: your implementation of the softmax function\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_prop: your implementation of the function to perform backward propagation\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    \n",
    "    iters = 0\n",
    "    C = 2 \n",
    "    total_cost = 0\n",
    "    \n",
    "    x_val, y_val = next(get_batches(data[-104:], word2Ind, V, C, 100))\n",
    "    \n",
    "    for x, y in get_batches(data[:train_data_size+1], word2Ind, V, C, batch_size):\n",
    "        # get z and h\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "\n",
    "        # get yhat\n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        # get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        total_cost += cost\n",
    "  \n",
    "        # get gradients\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # update weights and biases\n",
    "        W1 = W1 - alpha * grad_W1\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        iters +=1\n",
    "        \n",
    "        if iters % 100 == 0:\n",
    "            z_val, h_val = forward_prop(x_val, W1, W2, b1, b2)\n",
    "            yhat_val = softmax(z_val)\n",
    "            c = compute_cost(y_val, yhat_val, 100)\n",
    "            \n",
    "            print(f\"   iteration: {iters}  alpha: {alpha:.6f}  val cost: {c:.6f}  train cost: {total_cost / iters:.6f}\")\n",
    "        \n",
    "        if iters % 1000 == 0:\n",
    "            print_acc(W1, W2, b1, b2, train_data_size)\n",
    "            save_weights_to_disk(W1, W2, b1, b2, alpha=str(alpha), batch_size=str(batch_size))\n",
    "        \n",
    "        if iters == num_iters:\n",
    "            print()\n",
    "            print(f\"total train cost: {total_cost / num_iters:.6f}\")\n",
    "            break\n",
    "        \n",
    "        if iters % 1000 == 0:\n",
    "            if alpha_decay:\n",
    "                alpha *= 0.9\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2\n",
    "N = 512\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "\n",
    "train_data_size = 1000\n",
    "num_iters = 2000\n",
    "batch_size = 256\n",
    "alpha = 0.3\n",
    "alpha_decay = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "   iteration: 100  alpha: 0.300000  val cost: 9.944628  train cost: 8.305529\n",
      "   iteration: 200  alpha: 0.300000  val cost: 9.858220  train cost: 7.052567\n",
      "   iteration: 300  alpha: 0.300000  val cost: 10.105922  train cost: 6.347946\n",
      "   iteration: 400  alpha: 0.300000  val cost: 10.142179  train cost: 5.820738\n",
      "   iteration: 500  alpha: 0.300000  val cost: 9.999357  train cost: 5.393869\n",
      "   iteration: 600  alpha: 0.300000  val cost: 10.044606  train cost: 5.023227\n",
      "   iteration: 700  alpha: 0.300000  val cost: 10.104488  train cost: 4.694914\n",
      "   iteration: 800  alpha: 0.300000  val cost: 10.152153  train cost: 4.398485\n",
      "   iteration: 900  alpha: 0.300000  val cost: 10.126706  train cost: 4.127952\n",
      "   iteration: 1000  alpha: 0.300000  val cost: 10.197496  train cost: 3.879271\n",
      "\n",
      "y*yhat: 283.22273190002784\n",
      "Accuracy: 0.856\n",
      "\n",
      "Saving weights to disk\n",
      "   iteration: 1100  alpha: 0.300000  val cost: 10.180573  train cost: 3.650494\n",
      "   iteration: 1200  alpha: 0.300000  val cost: 10.196498  train cost: 3.440533\n",
      "   iteration: 1300  alpha: 0.300000  val cost: 10.237954  train cost: 3.248346\n",
      "   iteration: 1400  alpha: 0.300000  val cost: 10.250949  train cost: 3.072744\n",
      "   iteration: 1500  alpha: 0.300000  val cost: 10.273862  train cost: 2.912475\n",
      "   iteration: 1600  alpha: 0.300000  val cost: 10.301784  train cost: 2.766157\n",
      "   iteration: 1700  alpha: 0.300000  val cost: 10.327177  train cost: 2.632514\n",
      "   iteration: 1800  alpha: 0.300000  val cost: 10.344699  train cost: 2.510291\n",
      "   iteration: 1900  alpha: 0.300000  val cost: 10.369636  train cost: 2.398297\n",
      "   iteration: 2000  alpha: 0.300000  val cost: 10.383897  train cost: 2.295443\n",
      "\n",
      "y*yhat: 733.6278256900025\n",
      "Accuracy: 1.0\n",
      "\n",
      "Saving weights to disk\n",
      "\n",
      "total train cost: 2.295443\n"
     ]
    }
   ],
   "source": [
    "W1, W2, b1, b2 = initialize_model(N,V, random_seed=42) #W1=(N,V) and W2=(V,N)\n",
    "#W1, W2, b1, b2 = load_weights_from_disk(alpha=str(alpha), batch_size=str(batch_size))\n",
    "#print_acc(W1, W2, b1, b2, train_data_size)\n",
    "\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(W1, W2, b1, b2, data, word2Ind, N, V, num_iters, batch_size, train_data_size=train_data_size, alpha=alpha, alpha_decay=alpha_decay)\n",
    "\n",
    "weights = (W1.copy(), W2.copy(), b1.copy(), b2.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weights to disk\n"
     ]
    }
   ],
   "source": [
    "save_weights_to_disk(W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the word vectors here\n",
    "from matplotlib import pyplot\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n",
    "         'rich','happy','sad']\n",
    "\n",
    "embs = (W1.T + W2)/2.0\n",
    " \n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 2)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 4)\n",
    "pyplot.scatter(result[:, 3], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
