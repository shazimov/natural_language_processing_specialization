{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, tokenize and process the data\n",
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')\n",
    "with open('./data/shakespeare.txt') as f:\n",
    "    data = f.read()\n",
    "data = re.sub(r'[,.!?;-]', '',data)\n",
    "data = nltk.word_tokenize(data)\n",
    "data = [ ch.lower() for ch in data if ch.isalpha()]\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N, V)\n",
    "    \n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V, N)\n",
    "    \n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N, 1)\n",
    "    \n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V, 1)\n",
    "    \n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def softmax(z):\n",
    "    '''\n",
    "    Inputs: \n",
    "        z: output scores from the hidden layer\n",
    "    Outputs: \n",
    "        yhat: prediction (estimate of y)\n",
    "    '''\n",
    "    # Calculate yhat (softmax)\n",
    "    yhat = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    return yhat\n",
    "\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result\n",
    "\n",
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "\n",
    "    # Calculate h\n",
    "    h = np.dot(W1, x) + b1\n",
    "  \n",
    "    # Apply the relu on h, \n",
    "    # store the relu in h\n",
    "    h = relu(h)\n",
    "    \n",
    "    # Calculate z\n",
    "    z = np.dot(W2, h) + b2\n",
    "\n",
    "    return z, h\n",
    "\n",
    "# compute_cost: cross-entropy cost function\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "    \n",
    "    # cost function\n",
    "    epsilon = 0.000001\n",
    "    logprobs = np.multiply(np.log(yhat + epsilon), y)\n",
    "    cost = -1 / batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    # Compute l1 as W2^T (Yhat - Y)\n",
    "    # and re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient\n",
    "    l1 = np.dot(W2.T, yhat - y)\n",
    "\n",
    "    # Apply relu to l1\n",
    "    Sl1 = relu(l1)\n",
    "\n",
    "    # compute the gradient for W1\n",
    "    grad_W1 = np.dot(l1, x.T) / batch_size\n",
    "\n",
    "    # Compute gradient of W2\n",
    "    grad_W2 = np.dot(yhat - y, h.T) / batch_size\n",
    "    \n",
    "    # compute gradient for b1\n",
    "    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size\n",
    "\n",
    "    # compute gradient for b2\n",
    "    grad_b2 = np.sum(yhat - y, axis=1, keepdims=True) / batch_size\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    return np.sum(y==y_hat)/len(y)\n",
    "\n",
    "def print_acc(W1, W2, b1, b2):\n",
    "    print()\n",
    "    x, y = next(get_batches(data, word2Ind, V, C, len(data)-4))\n",
    "    z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "    yhat = softmax(z)\n",
    "    s = np.sum(yhat[:,0] == yhat[:,1])\n",
    "    if s != 0:\n",
    "        print(\"Error s != 0\")\n",
    "\n",
    "    s = y * yhat\n",
    "    print(\"y*yhat: \" + str(np.sum(s)))\n",
    "    \n",
    "    yhat_ind = np.argmax(yhat, axis=0)\n",
    "    y_ind = np.argmax(y, axis=0)\n",
    "    \n",
    "    acc = accuracy(y_ind, yhat_ind)\n",
    "    \n",
    "    print(\"Accuracy: \" + str(acc))\n",
    "    print()\n",
    "    \n",
    "def save_weights_to_disk(W1, W2, b1, b2, alpha='', batch_size=''):\n",
    "    print(\"Saving weights to disk\")\n",
    "    np.savetxt(f\"W1_{alpha}_{batch_size}.csv\", W1, delimiter=',')\n",
    "    np.savetxt(f\"W2_{alpha}_{batch_size}.csv\", W2, delimiter=',')\n",
    "    np.savetxt(f\"b1_{alpha}_{batch_size}.csv\", b1, delimiter=',')\n",
    "    np.savetxt(f\"b2_{alpha}_{batch_size}.csv\", b2, delimiter=',')\n",
    "\n",
    "def load_weights_from_disk(alpha='', batch_size=''):\n",
    "    print(\"Loading weights from disk\")\n",
    "    W1 = np.loadtxt(f\"W1_{alpha}_{batch_size}.csv\", delimiter=',')\n",
    "    W2 = np.loadtxt(f\"W2_{alpha}_{batch_size}.csv\", delimiter=',')\n",
    "    b1 = np.expand_dims(np.loadtxt(f\"b1_{alpha}_{batch_size}.csv\", delimiter=','), axis=1)\n",
    "    b2 = np.expand_dims(np.loadtxt(f\"b2_{alpha}_{batch_size}.csv\", delimiter=','), axis=1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(W1, W2, b1, b2, data, word2Ind, N, V, num_iters, batch_size, alpha, \n",
    "                     alpha_decay=False, random_seed=282, initialize_model=initialize_model, \n",
    "                     get_batches=get_batches, forward_prop=forward_prop, \n",
    "                     softmax=softmax, compute_cost=compute_cost, \n",
    "                     back_prop=back_prop):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: your implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_prop: your implementation of the function to perform forward propagation\n",
    "        softmax: your implementation of the softmax function\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_prop: your implementation of the function to perform backward propagation\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    \n",
    "    iters = 0\n",
    "    C = 2 \n",
    "    total_cost = 0\n",
    "    \n",
    "    x_val, y_val = next(get_batches(data[:104], word2Ind, V, C, 100))\n",
    "    \n",
    "    for x, y in get_batches(data[104:], word2Ind, V, C, batch_size):\n",
    "        # get z and h\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "\n",
    "        # get yhat\n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        # get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        total_cost += cost\n",
    "  \n",
    "        # get gradients\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # update weights and biases\n",
    "        W1 = W1 - alpha * grad_W1\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        iters +=1\n",
    "        \n",
    "        if iters % 100 == 0:\n",
    "            z_val, h_val = forward_prop(x_val, W1, W2, b1, b2)\n",
    "            yhat_val = softmax(z_val)\n",
    "            c = compute_cost(y_val, yhat_val, 100)\n",
    "            \n",
    "            print(f\"   iteration: {iters}\")\n",
    "            print(f\"   alpha: {alpha:.6f}\")\n",
    "            print(f\"   val cost:   {c:.6f}\")\n",
    "            print(f\"   train cost: {total_cost / iters:.6f}\")\n",
    "            print()\n",
    "        \n",
    "        if iters % 1000 == 0:\n",
    "            print_acc(W1, W2, b1, b2)\n",
    "            save_weights_to_disk(W1, W2, b1, b2, alpha=str(alpha), batch_size=str(batch_size))\n",
    "        \n",
    "        if iters == num_iters:\n",
    "            print()\n",
    "            print(f\"total train cost: {total_cost / num_iters:.6f}\")\n",
    "            break\n",
    "        \n",
    "        if iters % 1000 == 0:\n",
    "            if alpha_decay:\n",
    "                alpha *= 0.9\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2\n",
    "N = 512\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "\n",
    "num_iters = 10000\n",
    "batch_size = 256\n",
    "alpha = .3\n",
    "alpha_decay = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W1, W2, b1, b2 = initialize_model(N,V, random_seed=42) #W1=(N,V) and W2=(V,N)\n",
    "#W1, W2, b1, b2 = load_weights_from_disk(alpha='0.3', batch_size='256')\n",
    "#print_acc(W1, W2, b1, b2)\n",
    "\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(W1, W2, b1, b2, data, word2Ind, N, V, num_iters, batch_size, alpha=alpha, alpha_decay=alpha_decay)\n",
    "\n",
    "weights = (W1.copy(), W2.copy(), b1.copy(), b2.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_weights_to_disk(W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha: 0.1\n",
    "#y*yhat: 1020.9995649051946\n",
    "#Accuracy: 0.08372444823707381\n",
    "#total train cost: 6.540943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha: 0.3\n",
    "\n",
    "#batch size: 128\n",
    "#val cost:   6.883895\n",
    "#train cost: 6.108548\n",
    "#y*yhat: 1641.483304054504\n",
    "#Accuracy: 0.11704355921027113\n",
    "\n",
    "#batch size: 256\n",
    "#val cost:   6.937128\n",
    "#train cost: 6.073951\n",
    "#y*yhat: 1855.7868529756695\n",
    "#Accuracy: 0.12664365230208294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha: 0.500000\n",
    "#val cost:   6.823716\n",
    "#train cost: 6.049268\n",
    "#y*yhat: 1548.4969183230244\n",
    "#Accuracy: 0.10965439664869478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha: 1.0\n",
    "#val cost:   6.777105\n",
    "#train cost: 6.102779\n",
    "#y*yhat: 1172.2643192233247\n",
    "#Accuracy: 0.09115239905356658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the word vectors here\n",
    "from matplotlib import pyplot\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n",
    "         'rich','happy','sad']\n",
    "\n",
    "embs = (W1.T + W2)/2.0\n",
    " \n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 2)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 4)\n",
    "pyplot.scatter(result[:, 3], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
